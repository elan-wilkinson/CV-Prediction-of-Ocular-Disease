{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168dc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Intro to AI Group 3 Final Project - Using CV to Predict Ocular Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the paths to the dataset folders on my local machine.\n",
    "train_base_dir = \"C:\\\\Users\\\\elanw\\\\OneDrive\\\\Documents\\\\IntroToAI\\\\ocular_dataset\\\\train\"\n",
    "image_dir = \"C:\\\\Users\\\\elanw\\\\OneDrive\\\\Documents\\\\IntroToAI\\\\ocular_dataset\\\\preprocessed_images\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this once, to iterate through the examples in the csv, and reorganize the images into subfolders by label\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "ocular_data = pd.read_csv(\"C:\\\\Users\\\\elanw\\\\OneDrive\\\\Documents\\\\IntroToAI\\\\ocular_dataset\\\\full_df.csv\")\n",
    "\n",
    "categs = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
    "\n",
    "if not os.path.exists(train_base_dir):\n",
    "        os.mkdir(train_base_dir)\n",
    "\n",
    "for categ in categs:\n",
    "    if not os.path.exists(train_base_dir + \"\\\\\" + categ):\n",
    "        os.mkdir(train_base_dir + \"\\\\\" + categ)\n",
    "\n",
    "#Normal (N),\n",
    "#Diabetes (D),\n",
    "#Glaucoma (G),\n",
    "#Cataract (C),\n",
    "#Age related Macular Degeneration (A),\n",
    "#Hypertension (H),\n",
    "#Pathological Myopia (M),\n",
    "#Other diseases/abnormalities (O)\n",
    "\n",
    "for index, row in ocular_data.iterrows():\n",
    "    l_file = str(row['ID']) + \"_left.jpg\"\n",
    "    r_file = str(row['ID'])+ \"_right.jpg\"\n",
    "    if row['N'] == 1:\n",
    "        ldest = train_base_dir + \"\\\\N\\\\\"\n",
    "        rdest = train_base_dir + \"\\\\N\\\\\"\n",
    "    elif row['D'] == 1:\n",
    "        ldest = train_base_dir + \"\\\\D\\\\\"\n",
    "        rdest = train_base_dir + \"\\\\D\\\\\"\n",
    "    elif row['G'] == 1:\n",
    "        ldest = train_base_dir + \"\\\\G\\\\\"\n",
    "        rdest = train_base_dir + \"\\\\G\\\\\"\n",
    "    elif row['C'] == 1:\n",
    "        ldest = train_base_dir + \"\\\\C\\\\\"\n",
    "        rdest = train_base_dir + \"\\\\C\\\\\"\n",
    "    elif row['A'] == 1:\n",
    "        ldest = train_base_dir + \"\\\\A\\\\\"\n",
    "        rdest = train_base_dir + \"\\\\A\\\\\"\n",
    "    elif row['H'] == 1:\n",
    "        ldest = train_base_dir + \"\\\\H\\\\\"\n",
    "        rdest = train_base_dir + \"\\\\H\\\\\"\n",
    "    elif row['M'] == 1:\n",
    "        ldest = train_base_dir + \"\\\\M\\\\\"\n",
    "        rdest = train_base_dir + \"\\\\M\\\\\"\n",
    "    elif row['O'] == 1:\n",
    "        ldest = train_base_dir + \"\\\\O\\\\\"\n",
    "        rdest = train_base_dir + \"\\\\O\\\\\"\n",
    "    if \"normal\" in row['Left-Diagnostic Keywords'].lower():\n",
    "        ldest = train_base_dir + \"\\\\N\\\\\"\n",
    "    elif \"cataract\" in row['Left-Diagnostic Keywords'].lower():\n",
    "        ldest = train_base_dir + \"\\\\C\\\\\"\n",
    "    elif \"pathological myopia\" in row['Left-Diagnostic Keywords'].lower():\n",
    "        ldest = train_base_dir + \"\\\\C\\\\\"\n",
    "    if \"normal\" in row['Right-Diagnostic Keywords'].lower():\n",
    "        rdest = train_base_dir + \"\\\\N\\\\\"\n",
    "    elif \"cataract\" in row['Right-Diagnostic Keywords'].lower():\n",
    "        rdest = train_base_dir + \"\\\\C\\\\\"\n",
    "    elif \"pathological myopia\" in row['Right-Diagnostic Keywords'].lower():\n",
    "        rdest = train_base_dir + \"\\\\C\\\\\"    \n",
    "    if os.path.exists(image_dir + l_file):\n",
    "        shutil.copy(image_dir + l_file, ldest + l_file)\n",
    "    if os.path.exists(image_dir + r_file):\n",
    "        shutil.copy(image_dir + r_file, rdest + r_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This duplicates training data by creating a horizontally flipped version of each.\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "categs = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
    "for category in categs:\n",
    "    for file in os.listdir(train_base_dir + \"\\\\\" + category):\n",
    "        img = cv2.imread(train_base_dir + \"\\\\\" + category + \"\\\\\" + file)\n",
    "        flipped_img = cv2.flip(img, 1)\n",
    "        cv2.imwrite(train_base_dir + \"\\\\\" + category + \"\\\\\" + file[:-4] + \"_flipped.jpg\", flipped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes right images and flips them horizontally \n",
    "import cv2\n",
    "import os\n",
    "\n",
    "categs = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
    "for category in categs:\n",
    "    for file in os.listdir(train_base_dir + \"\\\\\" + category):\n",
    "        if file.__contains__(\"right\"):\n",
    "            img = cv2.imread(train_base_dir + \"\\\\\" + category + \"\\\\\" + file)\n",
    "            flipped_img = cv2.flip(img, 1)\n",
    "            cv2.imwrite(train_base_dir + \"\\\\\" + category + \"\\\\\" + file[:-4] + \".jpg\", flipped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base model for VGG16, using frozen pretrained weights from imagenet\n",
    "# reducing the image size to 224x224\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "base_model = VGG16(weights='imagenet',\n",
    "                   input_shape=(224, 224, 3),\n",
    "                   include_top=False)\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 2 dense layers and a prediction layer to the base model\n",
    "\n",
    "from tensorflow.keras import models, layers\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer = layers.Dense(80, activation='relu')\n",
    "dense_layer2 = layers.Dense(40, activation='relu')\n",
    "prediction_layer = layers.Dense(8, activation='softmax')\n",
    "\n",
    "xfer_vgg16_model = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer,\n",
    "    dense_layer2,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model, using sparse categorical crossentropy rather than categorial\n",
    "# because we have category labels that are exclusive from one another\n",
    "# and data cannot be in more than one category\n",
    "# Using Adam as the optimizer as experimentations running the model with SGD with various learning rates\n",
    "# had inferior performance\n",
    "xfer_vgg16_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Image Data Generator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=False, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5117 images belonging to 8 classes.\n",
      "Found 1275 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow images from directories with label data\n",
    "# Setting class mode to sparse categorical for sparse categorical crossentropy\n",
    "# Batch size 20-32 seems to perform well\n",
    "# Shuffle set to false for validation data, as .labels and .classes returns labels in order, not matching shuffle status\n",
    "# Keeping color mode rgb - vgg16 requires 3 input channels, and when using PIL and manually converting to grayscale,\n",
    "# the model performance was not improved\n",
    "train_it = train_datagen.flow_from_directory(train_base_dir, target_size=(224, 224), color_mode='rgb', class_mode='sparse', batch_size=32,  subset='training',  shuffle=True)\n",
    "valid_it = train_datagen.flow_from_directory(train_base_dir, target_size=(224, 224), color_mode='rgb', class_mode='sparse', batch_size=32,  subset='validation', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "160/160 [==============================] - 432s 3s/step - loss: 2.5349 - accuracy: 0.4565 - val_loss: 1.5414 - val_accuracy: 0.4894\n",
      "Epoch 2/20\n",
      "160/160 [==============================] - 415s 3s/step - loss: 1.2175 - accuracy: 0.5855 - val_loss: 1.7059 - val_accuracy: 0.4141\n",
      "Epoch 3/20\n",
      "160/160 [==============================] - 419s 3s/step - loss: 1.0260 - accuracy: 0.6400 - val_loss: 1.5657 - val_accuracy: 0.5059\n",
      "Epoch 4/20\n",
      "160/160 [==============================] - 424s 3s/step - loss: 0.8970 - accuracy: 0.6691 - val_loss: 1.5715 - val_accuracy: 0.4996\n",
      "Epoch 5/20\n",
      "160/160 [==============================] - 432s 3s/step - loss: 0.8138 - accuracy: 0.7041 - val_loss: 1.6819 - val_accuracy: 0.4761\n",
      "Epoch 6/20\n",
      "160/160 [==============================] - 417s 3s/step - loss: 0.7381 - accuracy: 0.7233 - val_loss: 1.6017 - val_accuracy: 0.4855\n",
      "Epoch 7/20\n",
      "160/160 [==============================] - 453s 3s/step - loss: 0.6762 - accuracy: 0.7473 - val_loss: 1.8076 - val_accuracy: 0.4698\n",
      "Epoch 8/20\n",
      "160/160 [==============================] - 421s 3s/step - loss: 0.6283 - accuracy: 0.7663 - val_loss: 1.7545 - val_accuracy: 0.5075\n",
      "Epoch 9/20\n",
      "160/160 [==============================] - 448s 3s/step - loss: 0.5438 - accuracy: 0.8013 - val_loss: 1.8292 - val_accuracy: 0.4965\n",
      "Epoch 10/20\n",
      "160/160 [==============================] - 56305s 354s/step - loss: 0.5151 - accuracy: 0.8054 - val_loss: 1.9839 - val_accuracy: 0.5176\n",
      "Epoch 11/20\n",
      "160/160 [==============================] - 437s 3s/step - loss: 0.4678 - accuracy: 0.8257 - val_loss: 2.0147 - val_accuracy: 0.4965\n",
      "Epoch 12/20\n",
      "160/160 [==============================] - 441s 3s/step - loss: 0.4009 - accuracy: 0.8460 - val_loss: 2.4551 - val_accuracy: 0.4431\n",
      "Epoch 13/20\n",
      "160/160 [==============================] - 448s 3s/step - loss: 0.4040 - accuracy: 0.8525 - val_loss: 2.3460 - val_accuracy: 0.5067\n",
      "Epoch 14/20\n",
      "160/160 [==============================] - 451s 3s/step - loss: 0.4248 - accuracy: 0.8370 - val_loss: 2.4939 - val_accuracy: 0.4447\n",
      "Epoch 15/20\n",
      "160/160 [==============================] - 459s 3s/step - loss: 0.4152 - accuracy: 0.8526 - val_loss: 2.9919 - val_accuracy: 0.4392\n",
      "Epoch 16/20\n",
      "160/160 [==============================] - 431s 3s/step - loss: 0.3439 - accuracy: 0.8728 - val_loss: 2.7268 - val_accuracy: 0.4980\n",
      "Epoch 17/20\n",
      "160/160 [==============================] - 428s 3s/step - loss: 0.3476 - accuracy: 0.8665 - val_loss: 2.8045 - val_accuracy: 0.4737\n",
      "Epoch 18/20\n",
      "160/160 [==============================] - 430s 3s/step - loss: 0.2920 - accuracy: 0.8876 - val_loss: 3.0124 - val_accuracy: 0.4949\n",
      "Epoch 19/20\n",
      "160/160 [==============================] - 432s 3s/step - loss: 0.2718 - accuracy: 0.9001 - val_loss: 2.8641 - val_accuracy: 0.4714\n",
      "Epoch 20/20\n",
      "160/160 [==============================] - 429s 3s/step - loss: 0.2528 - accuracy: 0.9044 - val_loss: 3.1381 - val_accuracy: 0.4949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a840ca5050>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model for 20 epochs\n",
    "xfer_vgg16_model.fit(train_it, epochs=20, validation_data=valid_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the model continuing to overfit to the training data, while validation accuracy hovers around 40% without improving. These results seem to be consistent regardless of color or greyscale or whether using the SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 81s 2s/step\n",
      "0.3819607843137255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = xfer_vgg16_model.predict(valid_it)\n",
    "score = accuracy_score(y_true=valid_it.classes, y_pred=predictions.argmax(axis=-1))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.11      0.15        53\n",
      "           1       0.34      0.56      0.42        64\n",
      "           2       0.43      0.47      0.45       424\n",
      "           3       0.37      0.22      0.28        68\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.49      0.69      0.57        51\n",
      "           6       0.47      0.31      0.37       420\n",
      "           7       0.23      0.37      0.28       178\n",
      "\n",
      "    accuracy                           0.38      1275\n",
      "   macro avg       0.32      0.34      0.32      1275\n",
      "weighted avg       0.40      0.38      0.38      1275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sklearn.metrics as metrics\n",
    "print(metrics.classification_report(valid_it.classes, predictions.argmax(axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that while the weakest performance is on the class with the fewest examples (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "160/160 [==============================] - 422s 3s/step - loss: 2.4235 - accuracy: 0.3311 - val_loss: 1.9876 - val_accuracy: 0.3451\n",
      "Epoch 2/20\n",
      "160/160 [==============================] - 418s 3s/step - loss: 1.8925 - accuracy: 0.3533 - val_loss: 1.9384 - val_accuracy: 0.3467\n",
      "Epoch 3/20\n",
      "160/160 [==============================] - 415s 3s/step - loss: 1.7782 - accuracy: 0.3682 - val_loss: 1.9625 - val_accuracy: 0.3631\n",
      "Epoch 4/20\n",
      "160/160 [==============================] - 420s 3s/step - loss: 1.7099 - accuracy: 0.3688 - val_loss: 1.8240 - val_accuracy: 0.3475\n",
      "Epoch 5/20\n",
      "160/160 [==============================] - 422s 3s/step - loss: 1.6983 - accuracy: 0.3512 - val_loss: 1.7884 - val_accuracy: 0.3373\n",
      "Epoch 6/20\n",
      "160/160 [==============================] - 423s 3s/step - loss: 1.6200 - accuracy: 0.3606 - val_loss: 1.8314 - val_accuracy: 0.3529\n",
      "Epoch 7/20\n",
      "160/160 [==============================] - 419s 3s/step - loss: 1.5580 - accuracy: 0.3766 - val_loss: 1.7786 - val_accuracy: 0.3412\n",
      "Epoch 8/20\n",
      "160/160 [==============================] - 413s 3s/step - loss: 1.5252 - accuracy: 0.3869 - val_loss: 1.7429 - val_accuracy: 0.3412\n",
      "Epoch 9/20\n",
      "160/160 [==============================] - 415s 3s/step - loss: 1.5080 - accuracy: 0.3856 - val_loss: 1.8555 - val_accuracy: 0.3357\n",
      "Epoch 10/20\n",
      "160/160 [==============================] - 414s 3s/step - loss: 1.5007 - accuracy: 0.3854 - val_loss: 1.6637 - val_accuracy: 0.3475\n",
      "Epoch 11/20\n",
      "160/160 [==============================] - 407s 3s/step - loss: 1.4905 - accuracy: 0.3825 - val_loss: 2.1233 - val_accuracy: 0.3365\n",
      "Epoch 12/20\n",
      "160/160 [==============================] - 406s 3s/step - loss: 1.4725 - accuracy: 0.3903 - val_loss: 1.9261 - val_accuracy: 0.3333\n",
      "Epoch 13/20\n",
      "160/160 [==============================] - 404s 3s/step - loss: 1.4389 - accuracy: 0.3971 - val_loss: 2.2881 - val_accuracy: 0.3349\n",
      "Epoch 14/20\n",
      "160/160 [==============================] - 405s 3s/step - loss: 1.4433 - accuracy: 0.3907 - val_loss: 2.0775 - val_accuracy: 0.3310\n",
      "Epoch 15/20\n",
      "160/160 [==============================] - 403s 3s/step - loss: 1.4324 - accuracy: 0.3993 - val_loss: 2.2709 - val_accuracy: 0.3443\n",
      "Epoch 16/20\n",
      "160/160 [==============================] - 401s 3s/step - loss: 1.4219 - accuracy: 0.4002 - val_loss: 1.7673 - val_accuracy: 0.3310\n",
      "Epoch 17/20\n",
      "160/160 [==============================] - 403s 3s/step - loss: 1.4137 - accuracy: 0.4057 - val_loss: 1.9073 - val_accuracy: 0.3396\n",
      "Epoch 18/20\n",
      "160/160 [==============================] - 403s 3s/step - loss: 1.4024 - accuracy: 0.4055 - val_loss: 2.0311 - val_accuracy: 0.3341\n",
      "Epoch 19/20\n",
      "160/160 [==============================] - 403s 3s/step - loss: 1.4084 - accuracy: 0.4026 - val_loss: 2.1996 - val_accuracy: 0.3420\n",
      "Epoch 20/20\n",
      "160/160 [==============================] - 404s 3s/step - loss: 1.3990 - accuracy: 0.4043 - val_loss: 2.1139 - val_accuracy: 0.3427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a840a870d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same attempt, but with VGG19\n",
    "import keras\n",
    "from tensorflow.keras.applications import VGG19\n",
    "base_model = VGG16(weights='imagenet',\n",
    "                   input_shape=(224, 224, 3),\n",
    "                   include_top=False)\n",
    "base_model.trainable = False\n",
    "from tensorflow.keras import models, layers\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer = layers.Dense(80, activation='relu')\n",
    "dense_layer2 = layers.Dense(40, activation='relu')\n",
    "prediction_layer = layers.Dense(8, activation='softmax')\n",
    "\n",
    "xfer_vgg19_model = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer,\n",
    "    dense_layer2,\n",
    "    prediction_layer\n",
    "])\n",
    "xfer_vgg19_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "xfer_vgg19_model.fit(train_it, epochs=20, validation_data=valid_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the accuracy score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "predictions = xfer_vgg19_model.predict(test_it)\n",
    "score = accuracy_score(y_true=test_it.classes, y_pred=predictions.argmax(axis=-1))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the classification report with precision, recall, and f1-score\n",
    "# and number of examples for each category\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "print(metrics.classification_report(test_it.classes, predictions.argmax(axis=-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful function to show an image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_image(image_path):\n",
    "    image = mpimg.imread(image_path)\n",
    "    plt.imshow(image)\n",
    "\n",
    "show_image('C:\\\\Users\\\\elanw\\\\OneDrive\\\\Pictures\\\\eye_square.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load image from path and preprocess it\n",
    "\n",
    "from tensorflow.keras.preprocessing import image as image_utils\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "def load_and_process_image(image_path):\n",
    "    img = image_utils.load_img(image_path, target_size=(224,224))\n",
    "    img = image_utils.img_to_array(img)\n",
    "    img = img.reshape(1,224,224,3)\n",
    "    img = preprocess_input(img)\n",
    "    return img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
