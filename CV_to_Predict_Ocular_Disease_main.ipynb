{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168dc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Intro to AI Group 3 Final Project - Using CV to Predict Ocular Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the paths to the dataset folders on my local machine.\n",
    "train_base_dir = \"C:\\\\Users\\\\elanw\\\\OneDrive\\\\Documents\\\\IntroToAI\\\\ocular_dataset\\\\train\"\n",
    "image_dir = \"C:\\\\Users\\\\elanw\\\\OneDrive\\\\Documents\\\\IntroToAI\\\\ocular_dataset\\\\preprocessed_images\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this once, to iterate through the examples in the csv, and reorganize the images into subfolders by label\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "ocular_data = pd.read_csv(\"C:\\\\Users\\\\elanw\\\\OneDrive\\\\Documents\\\\IntroToAI\\\\ocular_dataset\\\\full_df.csv\")\n",
    "\n",
    "categs = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
    "\n",
    "if not os.path.exists(train_base_dir):\n",
    "        os.mkdir(train_base_dir)\n",
    "\n",
    "for categ in categs:\n",
    "    if not os.path.exists(train_base_dir + \"\\\\\" + categ):\n",
    "        os.mkdir(train_base_dir + \"\\\\\" + categ)\n",
    "\n",
    "\n",
    "for index, row in ocular_data.iterrows():\n",
    "    l_file = str(row['ID']) + \"_left.jpg\"\n",
    "    r_file = str(row['ID'])+ \"_right.jpg\"\n",
    "    if row['N'] == 1:\n",
    "        dest = train_base_dir + \"\\\\N\\\\\"\n",
    "    elif row['D'] == 1:\n",
    "        dest = train_base_dir + \"\\\\D\\\\\"\n",
    "    elif row['G'] == 1:\n",
    "        dest = train_base_dir + \"\\\\G\\\\\"\n",
    "    elif row['C'] == 1:\n",
    "        dest = train_base_dir + \"\\\\C\\\\\"\n",
    "    elif row['A'] == 1:\n",
    "        dest = train_base_dir + \"\\\\A\\\\\"\n",
    "    elif row['H'] == 1:\n",
    "        dest = train_base_dir + \"\\\\H\\\\\"\n",
    "    elif row['M'] == 1:\n",
    "        dest = train_base_dir + \"\\\\M\\\\\"\n",
    "    elif row['O'] == 1:\n",
    "        dest = train_base_dir + \"\\\\O\\\\\"\n",
    "    if os.path.exists(image_dir + l_file):\n",
    "        shutil.copy(image_dir + l_file, dest + l_file)\n",
    "    if os.path.exists(image_dir + r_file):\n",
    "        shutil.copy(image_dir + r_file, dest + r_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This duplicates training data by creating a horizontally flipped version of each.\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "categs = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
    "for category in categs:\n",
    "    for file in os.listdir(train_base_dir + \"\\\\\" + category):\n",
    "        img = cv2.imread(train_base_dir + \"\\\\\" + category + \"\\\\\" + file)\n",
    "        flipped_img = cv2.flip(img, 1)\n",
    "        cv2.imwrite(train_base_dir + \"\\\\\" + category + \"\\\\\" + file[:-4] + \"_flipped.jpg\", flipped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes right images and flips them horizontally \n",
    "import cv2\n",
    "import os\n",
    "\n",
    "categs = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
    "for category in categs:\n",
    "    for file in os.listdir(train_base_dir + \"\\\\\" + category):\n",
    "        if file.__contains__(\"right\"):\n",
    "            img = cv2.imread(train_base_dir + \"\\\\\" + category + \"\\\\\" + file)\n",
    "            flipped_img = cv2.flip(img, 1)\n",
    "            cv2.imwrite(train_base_dir + \"\\\\\" + category + \"\\\\\" + file[:-4] + \".jpg\", flipped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base model for VGG16, using frozen pretrained weights from imagenet\n",
    "# reducing the image size to 224x224\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "base_model = VGG16(weights='imagenet',\n",
    "                   input_shape=(224, 224, 3),\n",
    "                   include_top=False)\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 2 dense layers and a prediction layer to the base model\n",
    "\n",
    "from tensorflow.keras import models, layers\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer = layers.Dense(80, activation='relu')\n",
    "dense_layer2 = layers.Dense(40, activation='relu')\n",
    "prediction_layer = layers.Dense(8, activation='softmax')\n",
    "\n",
    "xfer_vgg16_model = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer,\n",
    "    dense_layer2,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model, using sparse categorical crossentropy rather than categorial\n",
    "# because we have category labels that are exclusive from one another\n",
    "# and data cannot be in more than one category\n",
    "# Using Adam as the optimizer as experimentations running the model with SGD with various learning rates\n",
    "# had inferior performance\n",
    "xfer_vgg16_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Image Data Generator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=False, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5117 images belonging to 8 classes.\n",
      "Found 1275 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow images from directories with label data\n",
    "# Setting class mode to sparse categorical for sparse categorical crossentropy\n",
    "# Batch size 20-32 seems to perform well\n",
    "# Shuffle set to false for validation data, as .labels and .classes returns labels in order, not matching shuffle status\n",
    "# Keeping color mode rgb - vgg16 requires 3 input channels, and when using PIL and manually converting to grayscale,\n",
    "# the model performance was not improved\n",
    "train_it = train_datagen.flow_from_directory(train_base_dir, target_size=(224, 224), color_mode='rgb', class_mode='sparse', batch_size=32,  subset='training',  shuffle=True)\n",
    "valid_it = train_datagen.flow_from_directory(train_base_dir, target_size=(224, 224), color_mode='rgb', class_mode='sparse', batch_size=32,  subset='validation', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "160/160 [==============================] - 444s 3s/step - loss: 3.7867 - accuracy: 0.3774 - val_loss: 2.2720 - val_accuracy: 0.3765\n",
      "Epoch 2/20\n",
      "160/160 [==============================] - 428s 3s/step - loss: 1.4919 - accuracy: 0.5212 - val_loss: 2.1253 - val_accuracy: 0.4000\n",
      "Epoch 3/20\n",
      "160/160 [==============================] - 414s 3s/step - loss: 1.1671 - accuracy: 0.5732 - val_loss: 1.8064 - val_accuracy: 0.4306\n",
      "Epoch 4/20\n",
      "160/160 [==============================] - 430s 3s/step - loss: 1.0514 - accuracy: 0.6058 - val_loss: 1.8944 - val_accuracy: 0.3788\n",
      "Epoch 5/20\n",
      "160/160 [==============================] - 415s 3s/step - loss: 0.9144 - accuracy: 0.6488 - val_loss: 1.9676 - val_accuracy: 0.3898\n",
      "Epoch 6/20\n",
      "160/160 [==============================] - 414s 3s/step - loss: 0.8555 - accuracy: 0.6811 - val_loss: 2.0284 - val_accuracy: 0.3843\n",
      "Epoch 7/20\n",
      "160/160 [==============================] - 453s 3s/step - loss: 0.7879 - accuracy: 0.6918 - val_loss: 2.0337 - val_accuracy: 0.4329\n",
      "Epoch 8/20\n",
      "160/160 [==============================] - 412s 3s/step - loss: 0.7372 - accuracy: 0.7106 - val_loss: 2.1767 - val_accuracy: 0.4337\n",
      "Epoch 9/20\n",
      "160/160 [==============================] - 402s 3s/step - loss: 0.6244 - accuracy: 0.7598 - val_loss: 2.3776 - val_accuracy: 0.3953\n",
      "Epoch 10/20\n",
      "160/160 [==============================] - 410s 3s/step - loss: 0.5890 - accuracy: 0.7737 - val_loss: 2.3387 - val_accuracy: 0.4267\n",
      "Epoch 11/20\n",
      "160/160 [==============================] - 411s 3s/step - loss: 0.5877 - accuracy: 0.7751 - val_loss: 2.3829 - val_accuracy: 0.3906\n",
      "Epoch 12/20\n",
      "160/160 [==============================] - 422s 3s/step - loss: 0.5507 - accuracy: 0.7872 - val_loss: 2.6197 - val_accuracy: 0.4322\n",
      "Epoch 13/20\n",
      "160/160 [==============================] - 439s 3s/step - loss: 0.4720 - accuracy: 0.8202 - val_loss: 2.6084 - val_accuracy: 0.4188\n",
      "Epoch 14/20\n",
      "160/160 [==============================] - 466s 3s/step - loss: 0.4485 - accuracy: 0.8265 - val_loss: 2.6586 - val_accuracy: 0.4188\n",
      "Epoch 15/20\n",
      "160/160 [==============================] - 430s 3s/step - loss: 0.4720 - accuracy: 0.8267 - val_loss: 2.9686 - val_accuracy: 0.3671\n",
      "Epoch 16/20\n",
      "160/160 [==============================] - 420s 3s/step - loss: 0.4713 - accuracy: 0.8214 - val_loss: 3.2271 - val_accuracy: 0.4110\n",
      "Epoch 17/20\n",
      "160/160 [==============================] - 423s 3s/step - loss: 0.4052 - accuracy: 0.8542 - val_loss: 2.9771 - val_accuracy: 0.4086\n",
      "Epoch 18/20\n",
      "160/160 [==============================] - 424s 3s/step - loss: 0.4049 - accuracy: 0.8466 - val_loss: 3.3396 - val_accuracy: 0.3835\n",
      "Epoch 19/20\n",
      "160/160 [==============================] - 416s 3s/step - loss: 0.3116 - accuracy: 0.8837 - val_loss: 3.4925 - val_accuracy: 0.4008\n",
      "Epoch 20/20\n",
      "160/160 [==============================] - 408s 3s/step - loss: 0.3441 - accuracy: 0.8775 - val_loss: 3.8107 - val_accuracy: 0.3686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a839ab2350>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model for 20 epochs\n",
    "xfer_vgg16_model.fit(train_it, epochs=20, validation_data=valid_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the model continuing to overfit to the training data, while validation accuracy hovers around 40% without improving. These results seem to be consistent regardless of color or greyscale or whether using the SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 81s 2s/step\n",
      "0.3819607843137255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = xfer_vgg16_model.predict(valid_it)\n",
    "score = accuracy_score(y_true=valid_it.classes, y_pred=predictions.argmax(axis=-1))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.11      0.15        53\n",
      "           1       0.34      0.56      0.42        64\n",
      "           2       0.43      0.47      0.45       424\n",
      "           3       0.37      0.22      0.28        68\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.49      0.69      0.57        51\n",
      "           6       0.47      0.31      0.37       420\n",
      "           7       0.23      0.37      0.28       178\n",
      "\n",
      "    accuracy                           0.38      1275\n",
      "   macro avg       0.32      0.34      0.32      1275\n",
      "weighted avg       0.40      0.38      0.38      1275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sklearn.metrics as metrics\n",
    "print(metrics.classification_report(valid_it.classes, predictions.argmax(axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that while the weakest performance is on the class with the fewest examples (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "160/160 [==============================] - 422s 3s/step - loss: 2.4235 - accuracy: 0.3311 - val_loss: 1.9876 - val_accuracy: 0.3451\n",
      "Epoch 2/20\n",
      "160/160 [==============================] - 418s 3s/step - loss: 1.8925 - accuracy: 0.3533 - val_loss: 1.9384 - val_accuracy: 0.3467\n",
      "Epoch 3/20\n",
      "160/160 [==============================] - 415s 3s/step - loss: 1.7782 - accuracy: 0.3682 - val_loss: 1.9625 - val_accuracy: 0.3631\n",
      "Epoch 4/20\n",
      "160/160 [==============================] - 420s 3s/step - loss: 1.7099 - accuracy: 0.3688 - val_loss: 1.8240 - val_accuracy: 0.3475\n",
      "Epoch 5/20\n",
      "139/160 [=========================>....] - ETA: 44s - loss: 1.6981 - accuracy: 0.3516"
     ]
    }
   ],
   "source": [
    "# Same attempt, but with VGG19\n",
    "import keras\n",
    "from tensorflow.keras.applications import VGG19\n",
    "base_model = VGG16(weights='imagenet',\n",
    "                   input_shape=(224, 224, 3),\n",
    "                   include_top=False)\n",
    "base_model.trainable = False\n",
    "from tensorflow.keras import models, layers\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer = layers.Dense(80, activation='relu')\n",
    "dense_layer2 = layers.Dense(40, activation='relu')\n",
    "prediction_layer = layers.Dense(8, activation='softmax')\n",
    "\n",
    "xfer_vgg19_model = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer,\n",
    "    dense_layer2,\n",
    "    prediction_layer\n",
    "])\n",
    "xfer_vgg19_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "xfer_vgg19_model.fit(train_it, epochs=20, validation_data=valid_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the accuracy score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "predictions = xfer_vgg19_model.predict(test_it)\n",
    "score = accuracy_score(y_true=test_it.classes, y_pred=predictions.argmax(axis=-1))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the classification report with precision, recall, and f1-score\n",
    "# and number of examples for each category\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "print(metrics.classification_report(test_it.classes, predictions.argmax(axis=-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful function to show an image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_image(image_path):\n",
    "    image = mpimg.imread(image_path)\n",
    "    plt.imshow(image)\n",
    "\n",
    "show_image('C:\\\\Users\\\\elanw\\\\OneDrive\\\\Pictures\\\\eye_square.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load image from path and preprocess it\n",
    "\n",
    "from tensorflow.keras.preprocessing import image as image_utils\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "def load_and_process_image(image_path):\n",
    "    img = image_utils.load_img(image_path, target_size=(224,224))\n",
    "    img = image_utils.img_to_array(img)\n",
    "    img = img.reshape(1,224,224,3)\n",
    "    img = preprocess_input(img)\n",
    "    return img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
